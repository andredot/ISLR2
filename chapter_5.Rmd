---
title: "ISLR Ch.5"
output: github_document
editor_options: 
  markdown: 
    wrap: sentence
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook.
When you execute code within the notebook, the results appear beneath the code.

```{r, include = FALSE}
library(tidyverse)
library(ISLR2)
library(ggpubr)
library(GGally)
library(boot)
```

### ex. 1

**Using basic statistical properties of the variance, as well as single variable calculus, derive (5.6). In other words, prove that α given by (5.6) does indeed minimize Var(αX + (1 − α)Y)** First, derive variance formula, then set it equal to 0. It should be equal to (5.6)

### ex. 2

**We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.**

**(a) What is the probability that the first bootstrap observation is not the jth observation from the original sample? Justify your answer.** The probability that the first observation is the jth sample is 1/n, so (a) is 1-1/n for each j in n.

**(b) What is the probability that the second bootstrap observation is not the jth observation from the original sample?** 1-1/n

**(c) Argue that the probability that the jth observation is not in the bootstrap sample is (1 − 1/n)^n** Not to be in the bootstrap sample it is necessary that it is not in the first AND not in the second AND ... so those statements should be TRUE at the same time, thus deriving the (c) formula.

**(d) When n = 5, what is the probability that the jth observation is in the bootstrap sample?** It can be there 1,2,3,4 or 5 times (we could calculate those exact probabilities), only it can not be there 0 times, so it is easy to obtain that probability...

```{r}
p0 <- (1 - 1/5)^5
(1 - p0)
```

**(e) When n = 100, what is the probability that the jth observation is in the bootstrap sample?**

```{r}
n <- 100
p0 <- (1 - 1/n)^n
(1 - p0) #at least once
```

**(f) When n = 10, 000, what is the probability that the jth observation is in the bootstrap sample?**

```{r}
n <- 10000
p0 <- (1 - 1/n)^n
(1 - p0) #at least once
```

**(g) Create a plot that displays, for each integer value of n from 1 to 100, 000, the probability that the jth observation is in the bootstrap sample. Comment on what you observe.** We can see that the values rapidly decrease and tend to stabilize around 0.63

```{r}
p0 <- function(n) (1 - 1/n)^n

ggplot() +
  geom_line( aes( x = 1:100000,
                  y = 1 - p0(..x..) )) +
  scale_x_log10() +
  ylim(0.5, 1)
```

**(h) We will now investigate numerically the probability that a bootstrap sample of size n = 100 contains the jth observation. Here j = 4. We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample. Comment on the results obtained.** Same formula as in the previous point, probability tend to converge to 63%

```{r}
store <- rep(NA, 10000)
for(i in 1:10000){ 
  store[i] <- sum(sample(1:100 , rep=TRUE) == 4) > 0
}
mean(store)
```

### ex. 3

**We now review k-fold cross-validation. (a) Explain how k-fold cross-validation is implemented.** First, decide a k. Then, divide the train dataset in k subsets. For each kth subset, train the model on all the other subsets and calculate the error on the kth one. Once you have the error for each of the k subsets, average it to obtain the estimate of MSE.

**(b) What are the advantages and disadvantages of k-fold crossvalidation relative to:**

i. The validation set approach? Less bias since it uses a bigger training set
ii. LOOCV? Less variance since training many near-identical models as in LOOCV has higher variance than the one of less correlated models as in k-fold CV

### ex. 4

**Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.** Prediction implies a function that maps a training set to the estimate of the response value. We need to code (or to describe) that function that should be

```{r, eval = FALSE}
f <- function( training_set ) {
  #some calculation
  return(response)
  }
```

Then we decide the number t of times that we wan to create a bootstrap database, proceed to create the bootstrapped training_set by randomly selecting observations from the training set (with replacement) until we have a dataset of the same length of the original one, and proceed to calculate the estimate of the response.

Once we have a vector of all the estimated responses from all the bootstrapped databases we simply calculate the std.dev of it.

### ex. 5