---
title: "ISLR Ch.2"
output: github_document
editor_options: 
  markdown: 
    wrap: sentence
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook.
When you execute code within the notebook, the results appear beneath the code.

```{r, include = FALSE}
library(tidyverse)
library(ISLR2)
library(ggpubr)
library(MASS)
library(car)
library(dplyr)
```

### ex. 1

**Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.**

- p-value of "intercept" is the probability that if there were no relationships in Advertising data, the resulting intercept which represent sales (obtained by fitting a linear model with TV, radio and newspaper predictors) would be different from 0, when no money is spent on predictors
- p-value of TV/Radio/Newspaper represent the probability that if there was no relationship between sales and money spent on TV/Radio/Newspaper, the change in sales for each euro spent on TV/Radio/Newspaper advertisement would be different from 0
- in general the null hypothesis is that money spent on TV or Radio or Newspapers are not correlated to the average sales of the product measured in the database (thus every pattern we see in the dataset is random)
- I'm confident that a LR model represent the reality within an acceptable level of accuracy when TV and Radio are used to model sales. At the same time, I'm not sure that adding newspaper will increase that accuracy, possibly only adding complexity without improving results.

### ex. 2

**Carefully explain the differences between the KNN classifier and KNN regression methods**

- KNN-c and KNN-r both select the K observation using the same definition of "nearest", thus selecting for a x1,x2...xK point in space the K observations that minimize the squared distance from it
- KNN-c then classifies the observation as the class most represented in that set of observations, irrespective of distance from the point
- KNN-r computes the average of the response on all the K points selected, first summing all the y and then dividing them by k
- Since the average can only make sense in a continuous or discrete space (there is no certainty that the predicted result will be one of the classification classes), it can not be generalized for cases with categorical variables than can not be represented on a x-axis

### ex. 3

**Suppose we have a data set with five predictors, X1 = GPA, X2 = IQ, X3 = Level (1 for College and 0 for High School), X4 = Interaction between GPA and IQ, and X5 = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get βˆ0 = 50, ˆβ1 = 20, ˆβ2 =0.07, ˆβ3 = 35, ˆβ4 =0.01, ˆβ5 = −10**

(a) Which answer is correct, and why? 

- i. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates. **no**
- ii. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates. **no**
- iii. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough. **yes**
- iv. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough. **no**

**The effect of beta4 can be ignored, since both GPA and IQ are kept fixed, and so is the value of their interaction term.**

**The effect of beta5 is 0 when x3 is 0, so the equation for the salary can be reduced to**

```{r, eval = FALSE}
# BETA = combined effect of Beta0 + Beta1*x1 + Beta2*x2 + beta4*x4
salary_high_school = BETA + beta3*x3 + beta5*x1*x3 
                   = BETA # since x3 = 0
salary_college = BETA + beta3*x3 + beta5*x1*x3 
               = BETA + beta3 + beta5*x1 # since x3 = 1
```

**So, for small values of x1 (GPA), salary_college > salary_high_school . But since beta5 is negative, as x1 increases, salary_college will get progressively smaller**

**(b) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0.**

```{r}
(salary_student_1 = 50 + 20*4.0 + 0.07*110 + 35*1 + 0.01*4.0*110 - 10*4.0*1)
```

(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.

**Short premise: IQ is a random variable with mean 100 and st.dev = 10, so it is not a good idea to use it in a linear model without transforming it: the difference between 80 and 90 is not the same between 90 and 100. Not sure if this also is true for GPA**

**TLDR: False. Full answer: the beta4 coefficient inform us about how strong is the association between salary and the combination of IQ and GPA, which is in fact weak (but please note that with GPA>3.5 at least 1/3 of the effect of IQ on salary is given by the interaction term). But, nothing can be said on the evidence of this interaction, that can not be appreciated without prediction intervals (which combine both the confidence interval of our prediction and our uncertainty on the estimate of the true beta4 value)**

### ex. 4

**I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. Y = β0 + β1*X + β2*X^2 + β3*X^3 + ϵ **

(a) Suppose that the true relationship between X and Y is linear, i.e. Y = β0 + β1X + ϵ. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer

**Adding quadratic and cubic terms introduce a more flexible model (lower bias, higher variance), which can provide a better fit to the training data. It depends on how much the error term random fluctuation is modelled by the additional terms to the Linear Regression (as the error term approaches zero, the training error of a cubic model will be more than a linear one, when the true relationship is linear)**

(b) Answer (a) using test rather than training RSS.

**Test RSS will be lower for a linear model than the one of a cubic model, when the true relationship is linear**

(c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

**The more a true relationship is far from a linear one, the more a cubic model can reduce its bias with only a slight increase of variance. In training data, a cubic model outperforms the linear one...**

(d) Answer (c) using test rather than training RSS.

**...but in test error it depends on the real case scenario: how much is the cubic one a better fit to the real function than the linear one? And how much random noise have been incorporated in the coefficients (due to high variability of a more complex model)?**

### ex. 5

Help

### ex. 6

Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point (¯x, ¯y).

![Sketch of exercise](img/ch3_ex6_1.jpg)

### ex. 7

**It is claimed in the text that in the case of simple linear regression of Y onto X, the R2 statistic (3.17) is equal to the square of the correlation between X and Y (3.18). Prove that this is the case. For simplicity, you may assume that ¯x =¯y= 0**

Help

### ex. 8

**This question involves the use of simple linear regression on the Auto data set.**

**(a) Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output.**

```{r}
attach(Auto)
lm.fit <- lm(mpg ∼ horsepower)
summary(lm.fit)
```

For example:

- i. Is there a relationship between the predictor and the response? **Yes, the probability that the pattern we observe is random is very remote**
- ii. How strong is the relationship between the predictor and the response? **Even if the coefficient is near zero, the relationship is pretty strong because horspower can vary from 50 to 200, while mpg from 10 to 40**

```{r, echo = FALSE, error = FALSE}
plot(horsepower, mpg)
abline(lm.fit , lwd = 3, col = "red")
```

- iii. Is the relationship between the predictor and the response positive or negative? **Negative, because the coefficient is negative**
- iv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?

```{r}
(confidence_interval = predict(lm.fit , list(horsepower = 98), interval = "confidence") )
```

```{r}
(prediction_interval = predict(lm.fit , list(horsepower = 98), interval = "prediction") )
```

(c) Use the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit. **we can see that residuals are not randomly distributed around 0, but they show a residual pattern not explained by the regression**

```{r}
plot(lm.fit) #instead of plot(predict(lm.fit), residuals(lm.fit))
```

### ex. 9

**This question involves the use of multiple linear regression on the Auto data set.**

**(a) Produce a scatterplot matrix which includes all of the variables in the data set**

```{r}
# old method
# pairs(Auto)

# new method (inspired by GitHub)
auto <- Auto

custom_function = function(data, mapping, method = "loess", ...){
      p = ggplot(data = data, mapping = mapping) + 
      geom_point() + 
      geom_smooth(method=method, ...)
      
      p
    }

auto %>% 
  dplyr::select(-name) %>%
  GGally::ggpairs( lower = list(continuous = custom_function) )

```

**(b) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.**

```{r}
auto <- Auto
auto %>% 
  dplyr::select(-name) %>% 
  cor()
```

**(c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results.** 

```{r}
auto_num <- auto %>% 
  dplyr::select(-name) 

lm.fit <- lm(mpg ~ .,
             data = auto_num)

summary(lm.fit)
```

**Comment on the output. For instance: **

**i. Is there a relationship between the predictors and the response?** Yes, a relationship is definitely plausible
**ii. Which predictors appear to have a statistically significant relationship to the response?** If we set a threashold of 0.01, "displacement" "weight" "year" and "origin" are low enough to refuse the hyphotesis of no relationship between each of them and "mpg"
**iii. What does the coefficient for the year variable suggest?** Since it's positive, whenever year increases so does also mpg (and since the p-value is extremely low, we can be sure enough that there is a strong correlation between year and mpg). For each additional year, mpg increases by 0.75. Nevertheless, coefficient alone is not enough, considering also the error term, we can predict mpg with a standard error of ~7%.


**(d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?**

```{r}
plot(lm.fit)
```

- evidence of non-linearity of the data: there is a modest evidence of non-linearity of the data. The U-shape of it suggest a transformation of predictors (quadratic?) may improve the fit
- correlation of error terms: no evidence to support the claim that the error in prediction of a car could be influenced by the error of the previous one. But, it is plausible that for instance car from the same manufacturer could show a correlation of error terms
- non-constant variance of error terms: there is cleare evidence of heteroscedasticity in the residuals vs fitted plot, as the error increases progressively. A concave transformation of the response would be advisable (log or sqrt, possibly)
- outliers: all values are within 2 standard deviations, well withing tolerance ranges
- high-leverage point: point 14 has high-leverage but a low standardized residual, so it doesn't affect the linear regression significantly
- collinearity: can not be seen from diagnostic plot, but very likely (see (a) exercise)

**(e) Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?**

```{r}
#lm.fit.interact <- lm(mpg ~ cylinders + displacement *
#                            horsepower * weight * acceleration * 
#                            year * origin ,
#                      data = auto_num)

lm.fit.interact <- lm(mpg ~ cylinders +
                        displacement +
                        horsepower +
                        weight +
                        acceleration +
                        year +
                        origin +
                        horsepower:cylinders +
                        horsepower:origin +
                        displacement:origin +
                        horsepower:weight,
                      data = auto_num)
summary(lm.fit.interact)
```

First attempt: calculate all possible interaction terms (bad idea, no meaningful insight).
Second attempt: to decide the interaction term to add, I plotted every variable against each other, selecting the combination that behaves roughly in a non-linear way. All pairs analyzed show a suggestive p-value.

```{r}
plot(lm.fit.interact)
```

From the residuals vs fitted graph we can see how the additional interaction terms have captured most of the residual pattern in the first lm.

**(f) Try a few different transformations of the variables, such as log(X),
√X, X^2. Comment on your findings**

To select which variables to transform, I plotted every variable against the predictor (see the first column of ggpair plot in (a) ), trying to figure out which function to apply to linearize response:

- cylinders, displacement, horsepower and weight seem that they would benefit from a log transform
- acceleration and year are roughly linear so they don't need a transform
- origin seem to benefit from a x^2 transform
- regarding the 4 interaction terms, they were working before so it's difficult to understand if they should be adjusted

```{r}
lm.fit.trasform <- lm(mpg ~ log(cylinders) +
                        log(displacement) +
                        log(horsepower) +
                        log(weight) +
                        acceleration +
                        year +
                        I(origin^2) +
                        horsepower:cylinders +
                        horsepower:origin +
                        displacement:origin +
                        horsepower:weight,
                      data = auto_num)
summary(lm.fit.trasform)
```

The results are better than the lm.fit model but not significantly better than the lm.fit.interact one (and still with a persistent heteroscedasticity).

Nevertheless, 86% of the variance is explained by the LM, an improvement of ~5% over the simple linear model.

### ex. 10

**This question should be answered using the Carseats data set.**

**(a) Fit a multiple regression model to predict Sales using Price, Urban, and US.**

```{r}
carseats <- Carseats

lm.fit.a <- lm( Sales ~ Price + Urban + US,
                data = carseats)
summary(lm.fit.a)
```

**(b) Provide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!**

```{r}
ggplot(carseats) + geom_density( mapping = aes(x=Price))
```

Consider that the model explain only a modest fraction (<25%) of the total variance.

- coefficient of Price: slightly negative, but as can be seen from the plot price usually spans from 70 to 150 so change in Sales is not minimal. 
- coefficient of UrbanYES: slightly negative but the standard error term is way bigger than the effect of being in a Urban area (vs being not), making this predictor useless
- coefficient of USYES: Being in US moves the Sales 1.2 up, even if a standard error of 20% is observed, compared to being in another place

**(c) Write out the model in equation form, being careful to handle the qualitative variables properly.**

```{r, eval = FALSE}
Sales = (+13.043469) +
        Price * (-0.054459) +
        Urban * (-0.021916) + # Urban can be only 1 (UrbanYes) or 0 (UrbanNo)
        US *    (+1.200573)   # US can be only 1 (USYes) or 0 (USNo)
```

**(d) For which of the predictors can you reject the null hypothesis H0 : βj = 0?** For Price and US predictors

**(e) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.**

```{r}
# benchmark model (all predictors)
lm.fit.bm <- lm( Sales ~ .,
                data = carseats)
# summary(lm.fit.bm)

# exercise (e)
lm.fit.e <- lm( Sales ~ Price + US,
                data = carseats)
summary(lm.fit.e)

```

**(f) How well do the models in (a) and (e) fit the data?** Basically in the same way, minimal differences are found in adjusted R-squared (amount of explained variance) and in RSE.

**(g) Using the model from (e), obtain 95% confidence intervals for the coefficient(s)**

```{r}
confint(lm.fit.e)
```

**(h) Is there evidence of outliers or high leverage observations in the model from (e)?**

```{r}
plot(lm.fit.e)
```

Standardized errors and leverage are well withing acceptable ranges.

## Quick recap

```{r, eval = FALSE}

lm.fit <- lm(medv ∼ lstat , data = Boston)
summary(lm.fit)

coef(lm.fit) #instead of names(lm.fit)
confint(lm.fit)

predict(lm.fit , data.frame(lstat = (c(5, 10, 15))), interval = "prediction")

attach(Boston)
plot(lstat , medv)
abline(lm.fit , lwd = 3, col = "red")

par(mfrow = c(2, 2))
plot(lm.fit)
```

